{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff83ffe-c7d1-408f-9365-583348ca4741",
   "metadata": {},
   "source": [
    "## CSCI 632 ML Homework 7: Trees and Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209aff2-88b1-44b0-b11f-9e2a3c6582e4",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "* **Insert all code, plots, results, and discussion** into this Jupyter Notebook.\n",
    "* Your homework should be submitted as a **single Jupyter Notebook** (.ipynb file).\n",
    "* While working, you use Google Colab by uploading this notebook and performing work there. Once complete, export the notebook as a Jupyter Notebook (.ipynb) and submit it to **Blackboard.**\n",
    "\n",
    "You can answer mathematical questions either by:\n",
    "* using LaTeX in a markdown cell, or\n",
    "* pasting a scanned or photographed handwritten answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be038ae0",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 1\n",
    "\n",
    "Let X be the dataset at a node and $Y \\in \\text{labels}$ the class label.\n",
    "For a split $\\sigma$ (e.g., a threshold on a feature or a categorical partition):\n",
    "\n",
    "$$\\mathrm{IG}(X,\\sigma) \\;=\\; H(Y)\\;-\\;H(Y\\mid \\sigma),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$H(Y) \\;=\\; -\\sum_{y} P(Y=y)\\log_2 P(Y=y), \\qquad\n",
    "H(Y\\mid \\sigma)\\;=\\;\\sum_{b} \\Pr(B_b)\\, H\\!\\left(Y\\,\\big|\\,B_b\\right)\n",
    "\\;=\\;\\sum_{b} \\frac{|X_b|}{|X|}\\,H(Y_{X_b}).$$\n",
    "\n",
    "Here $B_b$ indexes the branches (children) produced by $\\sigma$, $X_b$ is\n",
    "the subset of $X$ in branch $b$, and $Y_{X_b}$ are the labels restricted\n",
    "to $X_b$. Use base-2 logs.  You can see examples in lecture 16's notes.\n",
    "\n",
    "Consider the case of an email spam detector:\n",
    "`Y = Spam?` (`+` = spam, `-` = not spam)\n",
    "\n",
    "$X$ has 14 items with class counts $(n_{+}, n_{-}) = (9,5)$.\n",
    "\n",
    "\n",
    "| id | HasAttachment | SenderDomainType | Y |\n",
    "|----|----------------|------------------|---|\n",
    "|  1 | 0 | free      | + |\n",
    "|  2 | 1 | free      | + |\n",
    "|  3 | 0 | free      | - |\n",
    "|  4 | 1 | free      | - |\n",
    "|  5 | 0 | corporate | + |\n",
    "|  6 | 0 | corporate | + |\n",
    "|  7 | 1 | corporate | + |\n",
    "|  8 | 0 | corporate | + |\n",
    "|  9 | 1 | corporate | - |\n",
    "| 10 | 0 | unknown   | + |\n",
    "| 11 | 0 | unknown   | + |\n",
    "| 12 | 0 | unknown   | - |\n",
    "| 13 | 1 | unknown   | + |\n",
    "| 14 | 1 | unknown   | - |\n",
    "\n",
    "\n",
    "$H(Y) = -(\\tfrac{9}{14} \\log_2 \\tfrac{9}{14} + \\tfrac{5}{14} \\log_2 \\tfrac{5}{14}) \\approx 0.9403$\n",
    "\n",
    "(a) Compute the entropy $H(Y|\\text{HasAttachment})$ and IG for the `HasAttachment` feature.\n",
    "\n",
    "(b) There are three ways to group sender domain types into a single binary decision: \n",
    "- $\\sigma_{\\texttt{free}}$:“In free?” vs “corporate or unknown”\n",
    "- $\\sigma_{\\texttt{corporate}}$: “In corporate?” vs “free or unknown”\n",
    "- $\\sigma_{\\texttt{unknown}}$: “In unknown?” vs “free or corporate”\n",
    "\n",
    "For each decision $\\sigma$ above, compute $H(Y\\mid \\sigma)$ and \n",
    "$\\mathrm{IG}(X,\\sigma)$. Report all three values.\n",
    "\n",
    "(c) Draw the binary decision tree for the feature-splits that maximizes IG at each interior\n",
    "node. Include the class posterior probabilities and class label for each leaf.  Stop\n",
    "when no split yields positive gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57753427",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Problem 2 \n",
    "\n",
    "Implement a binary decision tree classifier from scratch using only numpy and standard python libraries\n",
    "(no pandas, scikit-learn, etc.).\n",
    "\n",
    "We provide two CSV files in the class repo:\n",
    "- `spam_train.csv`\n",
    "- `spam_test.csv`\n",
    "\n",
    "Each row is an email with five features and a binary label:\n",
    "\n",
    "- `HasAttachment` ∈ {0,1}\n",
    "- `SenderDomainType` ∈ {free, corporate, unknown}\n",
    "- `NumLinks` (float)\n",
    "- `SubjectLength` (int, number of characters)\n",
    "- `IsWeekend` ∈ {0,1}\n",
    "- `Y` ∈ {0,1} where 1 = spam, 0 = not spam\n",
    "\n",
    "(a) Implement the following API:\n",
    "\n",
    "- `class DecisionTree:`\n",
    "  - `__init__(self, max_depth=None, min_leaf_size=1, random_state=None)`\n",
    "  - `fit(X, y)`\n",
    "  - `predict(X) -> np.ndarray[int]`\n",
    "\n",
    "(b) train a decision tree using `spam_train.csv` with default (infinite)\n",
    "`max_depth` and default `min_leaf_size` of 1.\n",
    "\n",
    "(c) measure the accuracy of your decision tree using `spam_train.csv`\n",
    "\n",
    "(d) Compare your accuracy to that of sklearn's tree.\n",
    "\n",
    "    from sklearn import tree\n",
    "\n",
    "Note: You can only use sklearn for (d).\n",
    "\n",
    "**Tie-break:** if IG ties, prefer based on alphabetical order of the feature name then minimum split threshold or first category name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aba9bd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Problem 3\n",
    "\n",
    "Implement a random forest classifier from scratch using only numpy and standard python libraries.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "- **Bootstrap sampling:** For each tree, draw a bootstrap sample *with replacement* from the training set. The bootstrap sample should have the same size as the training set.\n",
    "- **Feature subsampling (m_try):**\n",
    "  - Classification default: `m_try = max(1, floor(sqrt(p)))`, where `p` = number of features.\n",
    "  - At each split, consider only a uniformly random subset of `m_try` features (re-sampled per node).\n",
    "- **Number of trees:** Default `T = 50` (see `n_trees` parameter).\n",
    "- **Aggregation:**\n",
    "  - Class prediction = **majority vote** across trees.\n",
    "  - Class probability = **mean of leaf probabilities** for class 1 across trees.\n",
    "\n",
    "- **Reproducibility:** Accept `random_state` to seed RNG; use `numpy.random.Generator`.\n",
    "\n",
    "(a) Implement the following API:\n",
    "\n",
    "- `class RandomForest:`\n",
    "  - `__init__(self, n_trees=50, m_try=None, max_depth=None, min_leaf_size=1, random_state=None)`\n",
    "  - `fit(X, y)`\n",
    "  - `predict(X) -> np.ndarray[int]`\n",
    "\n",
    "(b) train a decision tree using `spam_train.csv`\n",
    "\n",
    "(c) measure the accuracy of your random forest classifier using `spam_train.csv`\n",
    "\n",
    "(d) Compare the performance of you implementation to sklearn's `RandomForestClassifier`:\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Note: You can only use sklearn for (d).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
