{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff83ffe-c7d1-408f-9365-583348ca4741",
   "metadata": {},
   "source": [
    "# CSCI 632 Machine Learning Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209aff2-88b1-44b0-b11f-9e2a3c6582e4",
   "metadata": {},
   "source": [
    "This homework provides a sampling of questions from last year's final, plus some\n",
    "other questions that cover topics that might appear on the final.\n",
    "It also covers the last bit of material in the class.\n",
    "\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "* **Insert all code, plots, results, and discussion** into this Jupyter Notebook.\n",
    "* Your homework should be submitted as a **single Jupyter Notebook** (.ipynb file).\n",
    "* While working, you use Google Colab by uploading this notebook and performing work there. Once complete, export the notebook as a Jupyter Notebook (.ipynb) and submit it to **Blackboard.**\n",
    "\n",
    "You can answer mathematical questions either by:\n",
    "* using LaTeX in a markdown cell, or\n",
    "* pasting a scanned or photographed handwritten answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89727f",
   "metadata": {},
   "source": [
    "### Part I: True / False\n",
    "\n",
    "1) True/False: The ReLU activation function is differentiable everywhere.\n",
    "\n",
    "2) True/False: Precision is the ratio of true positives to the total number of positive predictions.\n",
    "\n",
    "3) True/False: The logistic activation function limits output to the range (0,1).\n",
    "\n",
    "4) True/False: High accuracy guarantees good performance in a dataset with class imbalance.\n",
    "\n",
    "5) True/False: The softmax activation function is only used in binary classification tasks.\n",
    "\n",
    "#### Additional study\n",
    "\n",
    "Expect other questions about anything covered in a lecture. This includes but is not limited to:\n",
    "* optimal classifiers\n",
    "* Bayes' risk\n",
    "* GDA\n",
    "* logistic regression\n",
    "* information, information gain, self-entropy\n",
    "* decision trees\n",
    "* random forests\n",
    "* multi-layer perceptrons\n",
    "* backpropagation\n",
    "* MLP for multi-class classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c47bb",
   "metadata": {},
   "source": [
    "### Part II: Multiple Choice\n",
    "\n",
    "Throughout this exam $y$ refers to the output and $x$ to the input\n",
    "of a model.  NOTE: there is exactly one correct answer to each question.\n",
    "\n",
    "6) Which activation function is appropriate for a regression task where the target values span the nonnegative real line?\n",
    "\n",
    "  * (A) ReLU\n",
    "  * (B) Logistic\n",
    "  * (C) Tanh\n",
    "  * (D) None (use a linear activation)\n",
    "\n",
    "7) Which of the following is true about the difference between generative and discriminative models?\n",
    "\n",
    "  * (A) Generative models learn the posterior probability $P(y \\mid x)$, while discriminative models learn likelihood $P(x \\mid y)$.\n",
    "  * (B) Generative models model the likelihood $P(x \\mid y)$ and the class prior $P(y)$, while discriminative models directly estimate the posterior $P(y \\mid x)$.\n",
    "  * (C) Logistic regression is a generative model, while GDA is a discriminative model.\n",
    "  * (D) Generative models cannot be used for classification.\n",
    "\n",
    "8) What is the primary goal of the optimal Bayes‚Äô classifier?\n",
    "  * (A) To maximize the likelihood of the observed data.\n",
    "  * (B) To minimize the probability of misclassification.\n",
    "  * (C) To find the decision boundary that separates the classes perfectly.\n",
    "  * (D) To estimate the class priors  $P(y)$  accurately.\n",
    "\n",
    "9) Logistic regression‚Äôs decision boundary is:\n",
    "  * (A) Non-linear in the input space.\n",
    "  * (B) Linear in the input space.\n",
    "  * (C) Linear in the feature space only when using softmax.\n",
    "  * (D) Based on $P(x \\mid y)$.\n",
    "\n",
    "10) Gaussian Discriminant Analysis (GDA) assumes:\n",
    "  * (A) The class-conditional distributions  $P(x \\mid y)$  are Gaussian.\n",
    "  * (B) The covariances of the Gaussian distributions for all classes are identical.\n",
    "  * (C) The class priors  $P(y)$  are known or can be estimated.\n",
    "  * (D) All of the above.\n",
    "\n",
    "#### Additional study\n",
    "\n",
    "Expect other questions about anything covered in a lecture. This includes but is not limited to the same scope as shown in T/F."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b15e0b",
   "metadata": {},
   "source": [
    "### Part III: Classifier Performance\n",
    "\n",
    "A binary classifier $h$ predicts whether an image contains a car (positive class).\n",
    "\n",
    "| # |Image | Description      |$y$|$\\hat{y}$\n",
    "|---|------|------------------|---|---------\n",
    "|1  | üöó   | red car          | T |  T\n",
    "|2  | üöô   | SUV              | T |  F\n",
    "|3  | üöï   | taxi             | T |  F\n",
    "|4  | üöì   | police car       | T |  F\n",
    "|5  | üöå   | bus              | F |  F\n",
    "|6  | üöö   | delivery truck   | F |  F\n",
    "|7  | üö≤   | bicycle          | F |  F\n",
    "|8  | üõµ   | scooter          | F |  F\n",
    "|9  | üê∂   | dog              | F |  F\n",
    "\n",
    "11) Create a confusion matrix and place the number of samples that fall in \n",
    "each quadrant in the respective quadrant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95529acf",
   "metadata": {},
   "source": [
    "12) Compute recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b459e",
   "metadata": {},
   "source": [
    "13) Compute precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b74113",
   "metadata": {},
   "source": [
    "14) Compute accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aec4b6",
   "metadata": {},
   "source": [
    "### Part IV: Bayes' Law\n",
    "\n",
    "**Problem 15**: A screening test is used to detect a certain type of cancer. The test has the following characteristics:\n",
    "\n",
    "* If a person has the cancer, the test correctly gives a positive result 98% of the time.\n",
    "* If a person does not have the cancer, the test correctly gives a negative result 92% of the time.\n",
    "* This cancer is relatively rare and affects 2 out of every 1,000 people in the general population.\n",
    "\n",
    "A randomly selected person takes the test, and the result is positive.\n",
    "What is the probability that this person actually has the cancer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0e648",
   "metadata": {},
   "source": [
    "### Part V: Shannon-Entropy, Cross-Entropy, Self-Information\n",
    "\n",
    "**Problem 16**\n",
    "\n",
    "Consider a 6-sided die with the following probabilities:\n",
    "\n",
    "\\begin{align*}\n",
    "P(1) &= 0.1 \\\\\n",
    "P(2) &= 0.1 \\\\\n",
    "P(3) &= 0.2 \\\\\n",
    "P(4) &= 0.2 \\\\\n",
    "P(5) &= 0.2 \\\\\n",
    "P(6) &= 0.2\n",
    "\\end{align*}\n",
    "\n",
    "**(a)** Compute the Shannon entropy \\(H(X)\\) of a roll in bits.\n",
    "\n",
    "\n",
    "**(b)** What would the entropy be if the die were fair?\n",
    "\n",
    "**(c)** Briefly explain why the entropy changes when the die becomes biased.\n",
    "\n",
    "**Additional material**\n",
    "\n",
    "Review the cross-information problem in HW 6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a195dfa",
   "metadata": {},
   "source": [
    "### Part VI: Decision Trees and Random Forests\n",
    "\n",
    "**Problem 17** \n",
    "\n",
    "You are building a decision tree to predict whether a customer will **buy** a product\n",
    "(`Buy = Yes` or `Buy = No`) based on simple demographic features.\n",
    "\n",
    "You have data for 14 customers. The target variable \\(Y\\) = `Buy` has the following\n",
    "distribution:\n",
    "\n",
    "- 9 customers: `Buy = Yes`\n",
    "- 5 customers: `Buy = No`\n",
    "\n",
    "So:\n",
    "\n",
    "$$P(Y = \\text{Yes}) = \\frac{9}{14}, \\quad P(Y = \\text{No}) = \\frac{5}{14}.$$\n",
    "\n",
    "You are considering two different attributes for the root split:\n",
    "\n",
    "- `AgeGroup` with values: `Young`, `Middle`, `Old`\n",
    "- `Student` with values: `Yes`, `No`\n",
    "\n",
    "The data grouped by attribute values are:\n",
    "\n",
    "**Split by `AgeGroup`:**\n",
    "\n",
    "- `Young`: 5 customers  \n",
    "  - 2: `Buy = Yes`  \n",
    "  - 3: `Buy = No`\n",
    "- `Middle`: 4 customers  \n",
    "  - 3: `Buy = Yes`  \n",
    "  - 1: `Buy = No`\n",
    "- `Old`: 5 customers  \n",
    "  - 4: `Buy = Yes`  \n",
    "  - 1: `Buy = No`\n",
    "\n",
    "**Split by `Student`:**\n",
    "\n",
    "- `Student = Yes`: 6 customers  \n",
    "  - 5: `Buy = Yes`  \n",
    "  - 1: `Buy = No`\n",
    "- `Student = No`: 8 customers  \n",
    "  - 4: `Buy = Yes`  \n",
    "  - 4: `Buy = No`\n",
    "\n",
    "Use base-2 logarithms in all entropy calculations.\n",
    "\n",
    "\n",
    "**(a)** Compute the entropy of the target variable \\(H(Y)\\) before any split:\n",
    "\n",
    "$$H(Y) = - \\sum_{y \\in \\{\\text{Yes}, \\text{No}\\}} P(Y = y) \\log_2 P(Y = y).$$\n",
    "\n",
    "Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017b650",
   "metadata": {},
   "source": [
    "**(b)** Compute the conditional entropy $H(Y \\mid \\text{AgeGroup})$.\n",
    "\n",
    "That is,\n",
    "\n",
    "$$H(Y \\mid \\text{AgeGroup}) =\n",
    "\\sum_{a \\in \\{\\text{Young}, \\text{Middle}, \\text{Old}\\}}\n",
    "P(\\text{AgeGroup} = a) \\, H(Y \\mid \\text{AgeGroup} = a).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5fa3c",
   "metadata": {},
   "source": [
    "**(c)** Compute the conditional entropy $H(Y \\mid \\text{Student})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2456d0df",
   "metadata": {},
   "source": [
    "**(d)** Compute the information gain for each attribute.\n",
    "\n",
    "Which attribute should be chosen as the root split according to the information gain criterion?\n",
    "Briefly justify your answer using the values you computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c5fb3",
   "metadata": {},
   "source": [
    "**Part VII: Multi-Layer Perceptrons**\n",
    "\n",
    "Consider a Multi-Layer Perceptron\n",
    "  (MLP) with the following structure:\n",
    "\n",
    "* Input layer: 2 nodes.\n",
    "* Hidden layer: 2 nodes with ReLU activation $\\text{ReLU}(x) = \\max(0, x)$.\n",
    "* Output layer: 3 nodes with softmax activation.\n",
    "\n",
    "Given\n",
    "\n",
    "* Input\n",
    "\n",
    "$$\\mathbf{a}^{[0]} = \\mathbf{x} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\end{bmatrix}.$$\n",
    "\n",
    "* Weights and biases for the hidden layer ($\\ell=1$):\n",
    "\n",
    "$$W^{[1]} = \\begin{bmatrix}\n",
    "  0.5 & -1.0 \\\\\n",
    "  1.5 & 2.0\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b}^{[1]} = \\begin{bmatrix}\n",
    "  0.0 \\\\\n",
    "  -0.5\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "* Weights and biases for the output layer ($\\ell=2$):\n",
    "\n",
    "$$W^{[2]} = \\begin{bmatrix}\n",
    "2.0 & -1.0 \\\\\n",
    "-1.0 & 1.0 \\\\\n",
    "0.5 & 0.5\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b}^{[2]} = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.5 \\\\\n",
    "-0.5\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Problem 18** Draw this multi-layer perceptron labelling each node and the weights on each edge.   Draw each node as a box containing a summation symbol,\n",
    "pre-activation label, activation function symbol, and activation label.\n",
    "Show the bias into each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb83936",
   "metadata": {},
   "source": [
    "**Problem 19** Is this neural network most appropriate for regression\n",
    "across positive real numbers, regression across the entire real-line,\n",
    "binary classification, or multi-class classification?   Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb046df",
   "metadata": {},
   "source": [
    "**Problem 20** Compute the pre-activation values $\\mathbf{z}^{[1]}$ for the\n",
    "hidden layer $\\ell=1$:\n",
    "\n",
    "$$\\mathbf{z}^{[1]} = W^{[1]} \\cdot \\mathbf{x} + \\mathbf{b}^{[1]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837627b8",
   "metadata": {},
   "source": [
    "**Problem 21** Apply the ReLU activation to compute the hidden layer activations\n",
    "$\\mathbf{a}^{[1]}$:\n",
    "\n",
    "$$\\mathbf{a}^{[1]} = \\text{ReLU}(\\mathbf{z}^{[1]})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea1fa9",
   "metadata": {},
   "source": [
    "**Problem 22** Compute the pre-activation values\n",
    "$\\mathbf{z}^{[2]}$ for the output layer:\n",
    "\n",
    "$$\\mathbf{z}^{[2]} = W^{[2]} \\cdot \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c442d",
   "metadata": {},
   "source": [
    "**Problem 23** Apply the softmax activation to compute the output probabilities\n",
    "$\\mathbf{a}_{[2]}$:\n",
    "\n",
    "$$\\text{softmax}(z^{[2]}_i) = \\frac{e^{z^{[2]}_i}}{\\sum_{j=1}^3 e^{z^{[2]}_j}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ece27",
   "metadata": {},
   "source": [
    "**Problem 24** Which class does the network predict based on the softmax output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8755f",
   "metadata": {},
   "source": [
    "**Part VIII**\n",
    "\n",
    "You are tasked with deriving the gradient update rules for a\n",
    "three-layer neural network, where layer $\\ell=0$ is the inputs, with a\n",
    "ReLU activation function at hidden layer $\\ell=1$ and a scalar output\n",
    "$\\hat{y}$ in layer $\\ell=2$.  We apply a mean squared error loss\n",
    "function to the output when training the network.  There is only one\n",
    "node in the output layer.\n",
    "\n",
    "The network is structured as follows:\n",
    "\n",
    "**Layer $\\ell=0$ (Input Layer):** $\\mathbf{x} \\in \\mathbb{R}^{n}$ (1\n",
    "  input vector of n features).\n",
    "\n",
    "**Layer $\\ell=1$ (Hidden Layer):**\n",
    "\n",
    " * Weights: $W^{[1]} \\in \\mathbb{R}^{m \\times n}$\n",
    " * Bias: $\\mathbf{b}^{[1]} \\in \\mathbb{R}^{m}$\n",
    " * Activation: $ReLU: \\text{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "**Layer $\\ell=2$                                            (Output Layer):**\n",
    "\n",
    " * Weights: $W^{[2]} \\in \\mathbb{R}^{1 \\times m}$\n",
    " * Bias: $b^{[2]} \\in \\mathbb{R}$\n",
    "\n",
    "The forward pass equations are:\n",
    "\n",
    " * Hidden layer $\\ell=1$ pre-activation: $\\mathbf{z}^{[1]} = W^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]} = W^{[1]} \\mathbf{a}^{[0}] + \\mathbf{b}^{[1]}$\n",
    " * Hidden layer $\\ell=1$ activation: $\\mathbf{a}^{[1]} = \\text{ReLU}(\\mathbf{z}^{[1]})$\n",
    " * Output layer pre-activation: $z^{[2]} = W^{[2]} \\mathbf{a}^{[1]} + b^{[2]}$\n",
    " * Prediction: $\\hat{y} = z^{[2]}$\n",
    "\n",
    "The loss function is the mean squared error:\n",
    "\n",
    "$$L = \\frac{1}{2} (\\hat{y} - y)^2,$$\n",
    "\n",
    "where $y$ is the true output.\n",
    "\n",
    "\n",
    "**Proble 25** Draw this neural neural network labelling each node and the\n",
    "weights on each edge.  Draw each node as a box containing a summation\n",
    "symbol, pre-activation label, activation function symbol, and\n",
    "activation label.  Show the bias into each node. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e355f03",
   "metadata": {},
   "source": [
    "**Problem 26** Draw matrices showing the layout of coefficients using the same\n",
    "label names as were used in your drawing of the neural network.\n",
    "For example how our weights laid out in the matrices,\n",
    "$W_{11}, W_{12}, \\dots W_{mn}$. This is to demonstrate an\n",
    "understanding of how the diagram in $(a)$ is represented as\n",
    "matrices.  Show the input vector, weights and bias at each level\n",
    "and the output vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af91df3",
   "metadata": {},
   "source": [
    "**Problem 27** Is this network most appropriate for regression across\n",
    "positive real numbers, regression across the entire real-line,\n",
    "binary classification, or multi-class classification?  Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d208",
   "metadata": {},
   "source": [
    "**Problem 28**  Derive the gradient of the loss function $L$ with respect to the\n",
    "second layer weights $W^{[2]}$ and bias $b^{[2]}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515d6b8",
   "metadata": {},
   "source": [
    "**Problem 29** Derive the gradient of the loss function $L$ with respect to the first layer weights $W^{[1]}$ and bias $b^{[1]}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37adc0b1",
   "metadata": {},
   "source": [
    "**Problem 30** Write the weight update equations for both layers using gradient descent:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L,$$\n",
    "\n",
    "where $\\theta$ represents the weights and biases, and $\\alpha$ is the\n",
    "learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
